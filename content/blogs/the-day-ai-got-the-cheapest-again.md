---
title: "The Day AI Got the Cheapest Again"
date: 2025-01-27
author: "Tyler Houchin"
tags: ["AI", "Nvidia", "OpenAI", "Infrastructure"]
draft: false
---

Today's market reaction to DeepSeek's [R1 model](https://github.com/deepseek-ai/DeepSeek-R1) breakthrough was dramatic, to say the least. The Nasdaq dropped 3.1%, and Nvidia saw its stock plummet by 17% - the largest single-day market cap loss in Wall Street history. The narrative? A Chinese hedge fund's ability to train and run powerful AI models at a fraction of the cost threatens the entire AI ecosystem.

![DeepSeek R1 Benchmarks](/images/blogs/the-day-ai-got-the-cheapest-again/benchmarks.jpg)
*DeepSeek R1's performance benchmarks compared to leading models. AIME tests advanced mathematical reasoning, MATH-500 evaluates complex mathematical problem-solving, GPQA Diamond measures general knowledge and reasoning, LiveCode Bench tests coding ability, and CodeForces rates algorithmic problem-solving skills.*

Their R1 model, performing on par with today's state-of-the-art systems, signals another step in AI's march toward accessibility (though in AI world, nobody can stay at the top of a leaderboard for long - and that's a very very good thing). Also, the news about a high performing model from DeepSeek being trained for cheap was announced [last month](https://github.com/deepseek-ai/DeepSeek-V3/tree/main). R1 made it to the headlines though.

When we talk about models, there's an important distinction: DeepSeek released an open source model, which means anyone can download and run it on their own hardware. When companies (or consumers) use these AI models, they're paying for the computing power that runs them - the GPUs and infrastructure processing the data. That infrastructure can be owned and operated by US companies, keeping both the data and revenue on US soil. So while DeepSeek may have created an incredibly cost-efficient model, that doesn't mean money will flow to them - and it certainly doesn't mean we'll be spending less on AI overall.

Jevons paradox, which Microsoft CEO [Satya Nadella](https://x.com/satyanadella/status/1883753899255046301) pointed out today, has helped shape disucssions about AI adoption at [Kamiwaza](https://kamiwaza.ai/). When steam engines became more efficient, coal consumption didn't decrease - it skyrocketed. Lower costs unlocked new possibilities, new uses, new scale.

The exact same dynamic is playing out in AI. As I [wrote about reasoning models](https://www.tylerhouchin.com/blogs/entering-the-inference-era/) when OpenAI's o1 was released, the future of AI compute goes far beyond training costs. The real compute demands come from running these models. By 2025's standards, we're barely scratching the surface of AI compute usage.

Markets focus on headlines - a $6M training breakthrough grabs attention. But DeepSeek's achievement signals something bigger: when powerful technology becomes accessible, its use explodes. 

For Nvidia and other datacenter-related stocks, this means one thing: more demand, not less. The world won't need fewer GPUs - it will need them everywhere. Factory floors, hospital beds, traffic systems (they're already in our phones and cars). The infrastructure play isn't shrinking; it's expanding into every corner of the economy.

DeepSeek didn't just make AI cheaper - they proved what's possible. And what's possible is that AI is about to be everywhere.