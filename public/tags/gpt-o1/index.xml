<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GPT-O1 on tyler houchin</title>
    <link>http://localhost:64435/tags/gpt-o1/</link>
    <description>Recent content in GPT-O1 on tyler houchin</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 06 Oct 2024 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:64435/tags/gpt-o1/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Entering the Inference Era</title>
      <link>http://localhost:64435/blogs/entering-the-inference-era/</link>
      <pubDate>Sun, 06 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:64435/blogs/entering-the-inference-era/</guid>
      <description>&lt;p&gt;In the world of AI, one of the most transformative insights has been the concept of &lt;strong&gt;scaling laws&lt;/strong&gt;—the idea that increasing the amount of compute, data, and model size leads to predictable gains in model intelligence. This principle became evident as AI models like GPT-3 and GPT-4 evolved. For example, GPT-3, with an estimated training cost of $10-20 million, was a massive leap forward in natural language understanding. However, GPT-4, with a training cost of around $100 million, required nearly 40 times more energy and compute than GPT-3. In return, it exhibited significantly improved language comprehension, reasoning, and overall intelligence. This increase in raw compute was a direct factor in the model’s heightened capabilities, demonstrating how scaling laws play out in the realm of model pre-training.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
