<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Open Source on tyler houchin</title><link>https://www.tylerhouchin.com/tags/open-source/</link><description>Recent content in Open Source on tyler houchin</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Mon, 01 Dec 2025 12:00:00 -0500</lastBuildDate><atom:link href="https://www.tylerhouchin.com/tags/open-source/index.xml" rel="self" type="application/rss+xml"/><item><title>Why is China Flooding the World with Free AI?</title><link>https://www.tylerhouchin.com/blogs/why-is-china-flooding-the-world-with-free-ai/</link><pubDate>Mon, 01 Dec 2025 12:00:00 -0500</pubDate><guid>https://www.tylerhouchin.com/blogs/why-is-china-flooding-the-world-with-free-ai/</guid><description>&lt;p>Open Twitter on any given day and you&amp;rsquo;re bound to see some new LLM release. However, it seems for every update to GPT/Claude/Gemini, we get 10 updates (or net new releases) from Chinese labs. This began exciting - progress is progress. But it has started to feel eerie. Why is China doing this? Almost all of the improvements made in open source LLMs have been at the hands of the Chinese.&lt;/p></description></item><item><title>To Rent or Own Your AI Brains</title><link>https://www.tylerhouchin.com/blogs/to-rent-or-own-your-ai-brain/</link><pubDate>Wed, 07 May 2025 13:57:10 -0400</pubDate><guid>https://www.tylerhouchin.com/blogs/to-rent-or-own-your-ai-brain/</guid><description>&lt;p>Two years ago, when we were still in the prehistoric era of building with LLMs, everyone defaulted to GPT-4. It just worked. An awesome developer experience coupled with a model that is always at the bleeding edge of what&amp;rsquo;s possible is what catalyzed this boom of rapid AI-enabled apps to be developed and shipped.&lt;/p>
&lt;p>Open source models were for those deep in pushing the models to do specific things, but to get the level of intelligence that OpenAI provides via ~2min setup, you needed expensive hardware, time sunk into getting the models running. You couldn&amp;rsquo;t ship nearly as fast. For all of the AI tinkering that I did, if I had to configure spinning up models I would not have explored the problem space of LLMs nearly as much.











&lt;figure class="">

 &lt;div>
 &lt;img loading="lazy" alt="2023 Benchmarks" src=" /images/blogs/rent-or-own/july_2023_bench.png">
 &lt;/div>

 
 &lt;div class="caption-container">
 &lt;figcaption> July 2023 - Source: &lt;a href="https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/">https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/&lt;/a> &lt;/figcaption>
 &lt;/div>
 
&lt;/figure>&lt;/p></description></item></channel></rss>