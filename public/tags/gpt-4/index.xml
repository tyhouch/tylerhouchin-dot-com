<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>GPT-4 on tyler houchin</title><link>https://www.tylerhouchin.com/tags/gpt-4/</link><description>Recent content in GPT-4 on tyler houchin</description><generator>Hugo</generator><language>en-us</language><lastBuildDate>Sun, 06 Oct 2024 00:00:00 +0000</lastBuildDate><atom:link href="https://www.tylerhouchin.com/tags/gpt-4/index.xml" rel="self" type="application/rss+xml"/><item><title>Entering the Inference Era</title><link>https://www.tylerhouchin.com/blogs/entering-the-inference-era/</link><pubDate>Sun, 06 Oct 2024 00:00:00 +0000</pubDate><guid>https://www.tylerhouchin.com/blogs/entering-the-inference-era/</guid><description>&lt;p>In the world of AI, one of the most transformative insights has been the concept of &lt;strong>scaling laws&lt;/strong>—the idea that increasing the amount of compute, data, and model size leads to predictable gains in model intelligence. This principle became evident as AI models like GPT-3 and GPT-4 evolved. For example, GPT-3, with an estimated training cost of $10-20 million, was a massive leap forward in natural language understanding. However, GPT-4, with a training cost of around $100 million, required nearly 40 times more energy and compute than GPT-3. In return, it exhibited significantly improved language comprehension, reasoning, and overall intelligence. This increase in raw compute was a direct factor in the model’s heightened capabilities, demonstrating how scaling laws play out in the realm of model pre-training.&lt;/p></description></item></channel></rss>