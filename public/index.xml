<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>tyler houchin</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on tyler houchin</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 28 Jan 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>The Day AI Got the Cheapest Again</title>
      <link>http://localhost:1313/blogs/the-day-ai-got-the-cheapest-again/</link>
      <pubDate>Tue, 28 Jan 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/the-day-ai-got-the-cheapest-again/</guid>
      <description>&lt;p&gt;Yesterday&amp;rsquo;s market reaction to DeepSeek&amp;rsquo;s AI breakthrough was dramatic, to say the least. The Nasdaq dropped 3.1%, and Nvidia saw its stock plummet by 17% - the largest single-day market cap loss in Wall Street history. The narrative? A hedge fund&amp;rsquo;s ability to train powerful AI models at a fraction of the cost threatens the entire AI ecosystem.&lt;/p&gt;&#xA;&lt;p&gt;Their R1 model, performing on par with today&amp;rsquo;s state-of-the-art systems, signals another step in AI&amp;rsquo;s march toward accessibility (though in this fast-moving field, that benchmark will likely change soon - and that&amp;rsquo;s a good thing).&lt;/p&gt;</description>
    </item>
    <item>
      <title>The Speed Limit of Thought</title>
      <link>http://localhost:1313/blogs/the-speed-of-thought/</link>
      <pubDate>Mon, 16 Dec 2024 18:51:21 -0800</pubDate>
      <guid>http://localhost:1313/blogs/the-speed-of-thought/</guid>
      <description>&lt;!-- raw HTML omitted --&gt;&#xA;&lt;p&gt;A few days ago I was reading &lt;a href=&#34;https://www.science.org/doi/10.1126/sciadv.aaw2594?adobe_mc=MCMID%3D79215321435021380951106322257345571933%7CMCORGID%3D242B6472541199F70A4C98A6%2540AdobeOrg%7CTS%3D1734403951&#34;&gt;this research paper&lt;/a&gt; that completely changed how I think about language, AI, and the future of work. These scientists studied 17 different languages and discovered something incredible: every human language transmits information at almost exactly the same rate - about 39 bits per second.&lt;/p&gt;&#xA;&lt;p&gt;Looking at their data, I initially assumed some languages would be way more efficient than others. Maybe Mandarin would pack in way more meaning than English, or certain languages would just be fundamentally better at getting ideas across.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learn How to Learn</title>
      <link>http://localhost:1313/blogs/learn-how-to-learn/</link>
      <pubDate>Sat, 09 Nov 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/learn-how-to-learn/</guid>
      <description>&lt;p&gt;In the future, your ability to create software won&amp;rsquo;t be limited by your technical knowledge. It will be limited by how clearly you can think.&#xA;Computing started with physical machines - we had to manually arrange circuits and transistors to make anything happen. Then we invented ways to encode instructions using just 1s and 0s. Next came human-readable instructions in assembly language. Then high-level languages like Python that let us write code almost like English. Now we&amp;rsquo;re at LLMs that can turn plain English into working software. Each breakthrough moved us further from thinking like machines and closer to thinking like humans.&#xA;But here&amp;rsquo;s what&amp;rsquo;s really interesting: the next abstraction won&amp;rsquo;t be technical at all.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Maybe Simple Is All You Need</title>
      <link>http://localhost:1313/blogs/maybe-simple-is-all-you-need/</link>
      <pubDate>Mon, 21 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/maybe-simple-is-all-you-need/</guid>
      <description>&lt;p&gt;Enabling self-improvement, where LLMs can autonomously make themselves better, is becoming increasingly feasible in the near future. These language models are already writing prod code (either via copy/paste or Cursor or whoever Devin&amp;rsquo;s clients are), but we’re rapidly heading toward a future where they will also orchestrate entire workflows. This opens the door to the concept of &amp;lsquo;single-use software&amp;rsquo;: when the cost of producing software becomes so cheap that we just write code for everything. The problem isn’t a lack of tasks that could benefit from software, it’s that we lack the resources to develop customized solutions for every use case. Every industry is full of repetitive processes that could be optimized with code, but hiring a dev (or allocating the time of the devs you do have) just isn&amp;rsquo;t worth it. Advanced frameworks like CrewAI and AutoGen are pushing the boundaries of what&amp;rsquo;s possible in multi-agent systems, enabling role delegation, tool use, and task management. &lt;a href=&#34;https://www.loom.com/share/cae1aa1bcf4d4a76a2406f1314c23a85&#34;&gt;I&amp;rsquo;ve played around with CrewAI&lt;/a&gt; and GPT-4o/Claude were pretty good at helping get something running pretty fast, but they weren&amp;rsquo;t zero-shotting it.&lt;/p&gt;</description>
    </item>
    <item>
      <title>My New Project</title>
      <link>http://localhost:1313/projects/hey/</link>
      <pubDate>Tue, 08 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/projects/hey/</guid>
      <description>A short overview of my new project.</description>
    </item>
    <item>
      <title>Entering the Inference Era</title>
      <link>http://localhost:1313/blogs/entering-the-inference-era/</link>
      <pubDate>Sun, 06 Oct 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/entering-the-inference-era/</guid>
      <description>&lt;p&gt;In the world of AI, one of the most transformative insights has been the concept of &lt;strong&gt;scaling laws&lt;/strong&gt;—the idea that increasing the amount of compute, data, and model size leads to predictable gains in model intelligence. This principle became evident as AI models like GPT-3 and GPT-4 evolved. For example, GPT-3, with an estimated training cost of $10-20 million, was a massive leap forward in natural language understanding. However, GPT-4, with a training cost of around $100 million, required nearly 40 times more energy and compute than GPT-3. In return, it exhibited significantly improved language comprehension, reasoning, and overall intelligence. This increase in raw compute was a direct factor in the model’s heightened capabilities, demonstrating how scaling laws play out in the realm of model pre-training.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
