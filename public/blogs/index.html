<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="https://www.tylerhouchin.com//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://www.tylerhouchin.com//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://www.tylerhouchin.com//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="https://www.tylerhouchin.com//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="https://www.tylerhouchin.com//apple-touch-icon.png">

<link rel="alternate" type="application/rss+xml" href="https://www.tylerhouchin.com/blogs/index.xml" title="tyler houchin">
<meta name="description" content=""/>

<title>
    
    Blogs | tyler houchin
    
</title>

<link rel="canonical" href="https://www.tylerhouchin.com/blogs/"/>












<link rel="stylesheet" href="/assets/combined.min.7a7da93198dfce6f15598e04c2ac1fb816c1c43554340ef36fa025ed8f90a119.css" media="all">









  </head>

  

  
  
  

  <body class="auto">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">tyler houchin</h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small  bold ">
            <a href="/blogs" >
                /blogs
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        

<div class="list-container">

    
<div class="breadcrumbs">
    
    <a href="/"></a>
    <span class="breadcrumbs-separator"> > </span>
    
    <a class="breadcrumbs-current" href="/blogs/">Blogs</a>
</div>


    <h1>Blogs</h1>
    

    

    
    
    
    

    

    

    
    <div class="post-line">

    
    
    
    

    <p class="line-date">21 Oct 2024 </p>

    <div>
        <p class="line-title">
            <a href="/blogs/maybe-simple-is-all-you-need/">
                Maybe Simple Is All You Need
            </a>
        </p>

        
        <p class="line-summary"> <p>Enabling self-improvement, where LLMs can autonomously make themselves better, is becoming increasingly feasible in the near future. These language models are already writing prod code (either via copy/paste or Cursor or whoever Devin&rsquo;s clients are), but we’re rapidly heading toward a future where they will also orchestrate entire workflows. This opens the door to the concept of &lsquo;single-use software&rsquo;: when the cost of producing software becomes so cheap that we just write code for everything. The problem isn’t a lack of tasks that could benefit from software, it’s that we lack the resources to develop customized solutions for every use case. Every industry is full of repetitive processes that could be optimized with code, but hiring a dev (or allocating the time of the devs you do have) just isn&rsquo;t worth it. Advanced frameworks like CrewAI and AutoGen are pushing the boundaries of what&rsquo;s possible in multi-agent systems, enabling role delegation, tool use, and task management. <a href="https://www.loom.com/share/cae1aa1bcf4d4a76a2406f1314c23a85">I&rsquo;ve played around with CrewAI</a> and GPT-4o/Claude were pretty good at helping get something running pretty fast, but they weren&rsquo;t zero-shotting it.</p> </p>
        
    </div>
</div>
    

    

    
    <div class="post-line">

    
    
    
    

    <p class="line-date">06 Oct 2024 </p>

    <div>
        <p class="line-title">
            <a href="/blogs/entering-the-inference-era/">
                Entering the Inference Era
            </a>
        </p>

        
        <p class="line-summary"> <p>In the world of AI, one of the most transformative insights has been the concept of <strong>scaling laws</strong>—the idea that increasing the amount of compute, data, and model size leads to predictable gains in model intelligence. This principle became evident as AI models like GPT-3 and GPT-4 evolved. For example, GPT-3, with an estimated training cost of $10-20 million, was a massive leap forward in natural language understanding. However, GPT-4, with a training cost of around $100 million, required nearly 40 times more energy and compute than GPT-3. In return, it exhibited significantly improved language comprehension, reasoning, and overall intelligence. This increase in raw compute was a direct factor in the model’s heightened capabilities, demonstrating how scaling laws play out in the realm of model pre-training.</p> </p>
        
    </div>
</div>
    

    

    

</div>

      </main>
    </div>

    <footer>
      

    
    <p>Powered by
        <a href="https://gohugo.io/">Hugo</a>
        and
        <a href="https://github.com/tomfran/typo">tomfran/typo</a>
    </p>
    
    
    



    </footer>
    
  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>

</html>